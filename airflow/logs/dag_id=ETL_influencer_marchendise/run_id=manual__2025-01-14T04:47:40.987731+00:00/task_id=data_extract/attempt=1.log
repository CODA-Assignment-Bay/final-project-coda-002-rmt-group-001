[2025-01-14T04:47:41.717+0000] {taskinstance.py:1171} INFO - Dependencies all met for <TaskInstance: ETL_influencer_marchendise.data_extract manual__2025-01-14T04:47:40.987731+00:00 [queued]>
[2025-01-14T04:47:41.724+0000] {taskinstance.py:1171} INFO - Dependencies all met for <TaskInstance: ETL_influencer_marchendise.data_extract manual__2025-01-14T04:47:40.987731+00:00 [queued]>
[2025-01-14T04:47:41.724+0000] {taskinstance.py:1368} INFO - 
--------------------------------------------------------------------------------
[2025-01-14T04:47:41.724+0000] {taskinstance.py:1369} INFO - Starting attempt 1 of 2
[2025-01-14T04:47:41.724+0000] {taskinstance.py:1370} INFO - 
--------------------------------------------------------------------------------
[2025-01-14T04:47:41.733+0000] {taskinstance.py:1389} INFO - Executing <Task(BashOperator): data_extract> on 2025-01-14 04:47:40.987731+00:00
[2025-01-14T04:47:41.736+0000] {standard_task_runner.py:52} INFO - Started process 211 to run task
[2025-01-14T04:47:41.738+0000] {standard_task_runner.py:79} INFO - Running: ['***', 'tasks', 'run', 'ETL_influencer_marchendise', 'data_extract', 'manual__2025-01-14T04:47:40.987731+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/AirflowM3.py', '--cfg-path', '/tmp/tmp8oyn1ec1', '--error-file', '/tmp/tmp1bn6xvzx']
[2025-01-14T04:47:41.739+0000] {standard_task_runner.py:80} INFO - Job 2: Subtask data_extract
[2025-01-14T04:47:41.776+0000] {task_command.py:371} INFO - Running <TaskInstance: ETL_influencer_marchendise.data_extract manual__2025-01-14T04:47:40.987731+00:00 [running]> on host e211999066b4
[2025-01-14T04:47:41.863+0000] {taskinstance.py:1583} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=afif
AIRFLOW_CTX_DAG_ID=ETL_influencer_marchendise
AIRFLOW_CTX_TASK_ID=data_extract
AIRFLOW_CTX_EXECUTION_DATE=2025-01-14T04:47:40.987731+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2025-01-14T04:47:40.987731+00:00
[2025-01-14T04:47:41.864+0000] {subprocess.py:62} INFO - Tmp dir root location: 
 /tmp
[2025-01-14T04:47:41.865+0000] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'sudo -u *** python /opt/***/dags/extract.py']
[2025-01-14T04:47:41.871+0000] {subprocess.py:85} INFO - Output:
[2025-01-14T04:47:42.916+0000] {subprocess.py:92} INFO - WARNING: An illegal reflective access operation has occurred
[2025-01-14T04:47:42.918+0000] {subprocess.py:92} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/***/.local/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2025-01-14T04:47:42.918+0000] {subprocess.py:92} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2025-01-14T04:47:42.918+0000] {subprocess.py:92} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2025-01-14T04:47:42.918+0000] {subprocess.py:92} INFO - WARNING: All illegal access operations will be denied in a future release
[2025-01-14T04:47:43.165+0000] {subprocess.py:92} INFO - 25/01/14 04:47:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-01-14T04:47:43.250+0000] {subprocess.py:92} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2025-01-14T04:47:43.251+0000] {subprocess.py:92} INFO - Setting default log level to "WARN".
[2025-01-14T04:47:43.251+0000] {subprocess.py:92} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2025-01-14T04:47:47.418+0000] {subprocess.py:92} INFO - Traceback (most recent call last):
[2025-01-14T04:47:47.423+0000] {subprocess.py:92} INFO -   File "/opt/***/dags/extract.py", line 43, in <module>
[2025-01-14T04:47:47.425+0000] {subprocess.py:92} INFO -     extract(path)
[2025-01-14T04:47:47.427+0000] {subprocess.py:92} INFO -   File "/opt/***/dags/extract.py", line 36, in extract
[2025-01-14T04:47:47.427+0000] {subprocess.py:92} INFO -     data.toPandas().to_csv(f'{st_path}/P2M3_afif_makruf_data_raw.csv', index=False)
[2025-01-14T04:47:47.427+0000] {subprocess.py:92} INFO -   File "/home/***/.local/lib/python3.7/site-packages/pandas/core/generic.py", line 3482, in to_csv
[2025-01-14T04:47:47.427+0000] {subprocess.py:92} INFO -     storage_options=storage_options,
[2025-01-14T04:47:47.427+0000] {subprocess.py:92} INFO -   File "/home/***/.local/lib/python3.7/site-packages/pandas/io/formats/format.py", line 1105, in to_csv
[2025-01-14T04:47:47.427+0000] {subprocess.py:92} INFO -     csv_formatter.save()
[2025-01-14T04:47:47.428+0000] {subprocess.py:92} INFO -   File "/home/***/.local/lib/python3.7/site-packages/pandas/io/formats/csvs.py", line 243, in save
[2025-01-14T04:47:47.428+0000] {subprocess.py:92} INFO -     storage_options=self.storage_options,
[2025-01-14T04:47:47.428+0000] {subprocess.py:92} INFO -   File "/home/***/.local/lib/python3.7/site-packages/pandas/io/common.py", line 707, in get_handle
[2025-01-14T04:47:47.428+0000] {subprocess.py:92} INFO -     newline="",
[2025-01-14T04:47:47.428+0000] {subprocess.py:92} INFO - FileNotFoundError: [Errno 2] No such file or directory: '/opt/***/logs/data_staging/P2M3_afif_makruf_data_raw.csv'
[2025-01-14T04:47:47.884+0000] {subprocess.py:96} INFO - Command exited with return code 1
[2025-01-14T04:47:47.905+0000] {taskinstance.py:1902} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/bash.py", line 197, in execute
    f'Bash command failed. The command returned a non-zero exit code {result.exit_code}.'
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-01-14T04:47:47.909+0000] {taskinstance.py:1412} INFO - Marking task as UP_FOR_RETRY. dag_id=ETL_influencer_marchendise, task_id=data_extract, execution_date=20250114T044740, start_date=20250114T044741, end_date=20250114T044747
[2025-01-14T04:47:47.920+0000] {standard_task_runner.py:97} ERROR - Failed to execute job 2 for task data_extract (Bash command failed. The command returned a non-zero exit code 1.; 211)
[2025-01-14T04:47:47.974+0000] {local_task_job.py:156} INFO - Task exited with return code 1
[2025-01-14T04:47:47.995+0000] {local_task_job.py:279} INFO - 0 downstream tasks scheduled from follow-on schedule check
